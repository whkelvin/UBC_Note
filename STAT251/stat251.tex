\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{float}
\usepackage{gensymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{sectsty}
\usepackage{indentfirst}
\usepackage{enumitem}

%\setlength{\parskip}{1em}

\definecolor{color:background}{RGB}{40,40,40}
\definecolor{color:text}{RGB}{230,230,230}

\pagecolor{color:background}
\color{color:text}
\allsectionsfont{\normalfont\sffamily\bfseries}

\title{STAT251 Elementary Statistics}
\author{Kelvin Hsu}


\begin{document}
    \sffamily
       \maketitle
       \newpage
    \section{Introduction to Statistic}
    Statistic is a science involving the design of studies, data collection , summarizing 
    data and analyzing data, interpreting results and drawing conclusions.
    \section*{Key Statistics Concepts}
    \begin{itemize}
        \item Population: all subject of interest in a study
        \item Sample: a subset of population
        \item Parameter: a descriptive measure of a population
        \item Statistic: a descriptive measure of a sample
        \item Census: collecting data for the entire population
        \item Sample Survey: collecting data for a sample
    \end{itemize}

    \section*{Classification of Variables}
    \begin{itemize}
        \item Categorical: gender, major, blood type...
        \item Quantitative: can be discrete or continuous
    \end{itemize}

    \section*{Inferential VS Descriptive Statistic}
    \begin{itemize}
        \item Inferential Statistic: Methods for making decision or prediction about a population based on 
        data collected on a sample.
        \item Descriptive Statistic: Methods of summarizing data (graphs and numbers)
    \end{itemize}

    \section*{Types of Graphs}
    For qualitative data, 
    \begin{itemize}
        \item Pie Chart
        \item Bar Graph
    \end{itemize}

    For quantitative data, 
    \begin{itemize}
        \item Dot Plot
        \item Stem and Leaf Plot
        \item Histogram
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/pie_chart.PNG}
        \caption{Pie Chart}
        \label{fig:pie_chart}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/bar_graph.PNG}
        \caption{Bar Graph}
        \label{fig:bar_graph}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/dot_graph.PNG}
        \caption{Dot Plot}
        \label{fig:dot_graph}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/histogram.PNG}
        \caption{Histogram}
        \label{fig:Histogram}
    \end{figure}
   
    \section*{Describing A Distribution}
    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/types_of_distribution.PNG}
        \caption{Types Of Distribution}
        \label{fig:types_of_distribution}
    \end{figure}

    \subsection*{Shape}
    \begin{itemize}
        \item Skewness: whether the distribution is symmetrical
        \item Center: where do the observation cluster about?
        \item Spread: Assess the spread of a distribution
        \item Outlier: observations that fall far from the rest of the data
        \item Mode: the most frequent observation
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/Skewness.PNG}
        \caption{Skewness}
        \label{fig:skewness}
    \end{figure}

    \section*{Measures of Center}
    \subsection*{Mean}
    Mean is the average of the observation.
    \begin{equation*}
        \bar{x} = \frac{\sum_{i = 1}^{n} x_{i}}{n}
    \end{equation*}
    where n is the number of observations.

    \subsection*{Median}
    Median is the midpoint of the observations.
    If the number of sample n is\dots
    \begin{align*}
        &\text{odd:}\\ 
        &\text{median is the midpoint} \frac{n+1}{2}\text{th point in an ordered list} \\
        &\text{even:}\\  
        &\text{median is the average of the 2 middle observations } (\frac{n}{2}th + \frac{n+1}{2}th) / 2
    \end{align*}

    \subsection*{Comparing Mean and Median}
    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/mean_and_median.PNG}
        \caption{Mean And Median}
        \label{fig:mean_and_median}
    \end{figure}

    \section*{Measure of Variability}
    Measure of variation gives information on the spread or variability or dispersion of the data.
    \subsection*{Range}
        \begin{equation*} 
            \text{Range} = X_{max} - X_{min}
        \end{equation*}
    
    \subsection*{Variance and Standard Deviation}
    \begin{align*}
        &\text{Variance} = S^{2} = \frac{ \sum_{i = 1}^{n} (x_{i}- \bar{x})^{2}} {n-1}\\
        &\text{Standard Deviation} = S = \sqrt{\frac{ \sum_{i = 1}^{n} (x_{i}- \bar{x})^{2}} {n-1}}\\
    \end{align*}
    Standard deviation measures the spread of data and it is zero only when all observations have the same value.
    As spread increases, s increases. S is sensitive to outliers.

    \section*{Interquartile Range}
    \begin{equation*}
        \text{IQR = Q3 - Q1}
    \end{equation*}
    \begin{itemize}
        \item Q1: first quartile, 25th percentile, the value in the sample that has $25\%$ of data below it
        \item Q2: median
        \item Q3: third quartile, 75th percentile
    \end{itemize}

    \section*{Identifying Outliers}
    If data is 1.5 IQR below Q1 or above Q3, then it is an outlier.

    \section*{Percentile}
    The n-th percentile is a value such that n percent of the observations fall below that value.

    \section*{Sample Quantile}
    Let $0<p<1$. The sample quantile or order p, Q(p) is a number with the property that approximately $p*100\%$ 
    of the data are below it.

    \begin{enumerate}
        \item Sort data from smallest to largest
        \item let m = $n*p + 0.5$
    \end{enumerate}
    if m is an integer:
    \begin{equation*}
        Q(p) = x_{m}
    \end{equation*}
    else:
    \begin{equation*}
        Q(p) = \frac{x_{m} + {x_{m+1}}}{2}
    \end{equation*}
    
    \section*{Box Plot}
    Box plot provides a way of looking at data to determine its central tendency, spread, skewness,
    and the existence of outliers.
    To construct a box plot,
    \begin{itemize}
        \item largest value within Q3 + IQR
        \item smallest value within Q1 - IQR
        \item Q1, Q2, and Q3
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=10cm]{figures/box_plot.PNG}
        \caption{Box Plot}
        \label{fig:box_plot}
    \end{figure}

    \section{Probability}
    \section*{Sample Space}
    Sample space is a set of all possible outcomes of a random experiment.

    \section*{Event}
    An event is a subset of a sample space

    \section*{Probability of a sample space}
    Each outcome has a possibility in a sample space. The probability for each outcome is between 0 an 1.
    The sum of all possibility = 1.

    \section*{Probability of an event}
    The probability of an event A, denoted by P(A), is obtained by adding the probability of 
    the individual outcomes in an event.
    \begin{itemize}
        \item  $0\leq P(A) \leq 1$
        \item P(A) = 1 implies A always occurs
        \item P(A) = 0 implies A never occurs
    \end{itemize}
    When all possible outcomes are equally likely,
        \begin{equation*}
            P(A) = \frac{\text{\# of outcomes in A}}{\text{\# of outcomes in sample space}}
        \end{equation*}

    \section*{Set Theory for Event using Venn Diagram}
    \subsection*{Complement of an Event}
        \begin{equation*}
            P(A^{c}) + P(A) = 1
        \end{equation*}
        \begin{figure}[H]
            \centering
            \includegraphics[width=10cm]{figures/complement.PNG}
            \caption{Complement of an event}
            \label{fig:complement}
        \end{figure}

    \subsection*{Intersection of 2 Events}
        \begin{figure}[H]
            \centering
            \includegraphics[width=10cm]{figures/intersection.PNG}
            \caption{Intersection of 2 Events}
            \label{fig:intersection}
        \end{figure}

    \subsection*{Disjoint or Mutually Exclusive Event}
        \begin{figure}[H]
            \centering
            \includegraphics[width=10cm]{figures/disjoint.PNG}
            \caption{Disjoint Event}
            \label{fig:disjoint}
        \end{figure}
   
    \subsection*{Union of 2 Events}
        \begin{figure}[H]
            \centering
            \includegraphics[width=10cm]{figures/union.PNG}
            \caption{Union of 2 Events}
            \label{fig:union}
        \end{figure}
 

    \subsection*{Property of Probability}
    \begin{align*}
        P(A\cup B) = P(A) + P(B) - P(A\cap B)\\
        P(A\cup B) = P(A) + P(B) \text{ if A and B are disjoint}\\
    \end{align*}

    \section*{Conditional Probability}
    Conditional probability is used to determine how 2 events are related. 
    \begin{align*}
        &P(A|B) = \frac{P(A\cap B)}{P(B)}\\
        &P(A\cap B) = P(B) \cdot P(A|B)\\
        &P(B|A) = \frac{P(A\cap B)}{P(A)}\\
        &P(A\cap B) = P(A) \cdot P(B|A)
    \end{align*}

    \section*{Independent Event}
    Event A and Event B are independent if knowing one event occurs does not change the probability of 
    the other.
    \begin{align*}
        P(A|B) = P(A)\\
        P(B|A) = P(B)\\
        P(A\cap B) = P(A)P(B) \text{ if A and B are independent}
    \end{align*}

    If A and B are independent, $A^{c}$ and $B$, $A$ and $B^{c}$, $A^{c}$ and $B^{c}$ are also 
    independent.

    \section*{Tree Diagram}
        \begin{figure}[H]
            \centering
            \includegraphics[width=10cm]{figures/tree.PNG}
            \caption{Tree Diagram}
            \label{fig:tree}
        \end{figure}
   
    \section{Random Variable}
    A random variable X is a function defined on the sample space S, assigning a number $x = X(\omega)$ to each outcome $\omega$ 
    in the sample space. Random variables are  defined by upper case letters such as X, Y, Z... Lower case values x, y, z represents a possible value of the 
    random variable

    \section*{Types of Random Variable}
    \begin{itemize}
        \item Discrete
        \item Continuous
    \end{itemize}

    Note
    \begin{itemize}
        \item For a continuous random variable X, $P(X = c) = 0$ for any c.
        \item Any random variable whose possible values are either 1 or 0 is called Bernoulli's Random Variable
    \end{itemize}

    \section*{Discrete Random Variable}
    \subsection*{Probability Mass Function (pmf)}
        \begin{equation*}
            f(x)= P(X=x)\\
        \end{equation*}
        where f(x) gives the probability of each possible value x of X. It has the following properties.

        \begin{align*}
            f(x) \geq 0 \text{ for all x}\\
            \sum f(x) =1
        \end{align*}

    \subsection*{Cumulative Distribution Function (cdf)}
        \begin{equation*}
            F(x) = P(X \leq x) = \sum_{d\leq x} f(k) \text{ for all real x}
        \end{equation*}
    \subsection*{Mean and Variance of a Discrete Random Variable}
        Mean(Expected Value) of a discrete random variable with pmf f(x) is 
        \begin{equation*}
            \mu = R[X] = \sum x f(x)
        \end{equation*}
        Variance of a discrete random variable is 
        \begin{align*}
            \sigma^{2} = Var(X) = E[(x-\mu)^{2}]\\
            Var(X) = E[x^{2}] - E[x] ^{2}
        \end{align*}

        The expected value of some function g(x) corresponding to the random variable X with pmf f(x) is 
        \begin{equation*}
            E[g(x)] = \sum g(x)f(x)
        \end{equation*}

    \section*{Continuous Random Variable}
    \subsection*{Probability Density Function}
    Let X be a continuous random variable, then the probability density function of X is a function f(x) such 
    that for any 2 numbers a and b and $a < b$.
    \begin{align*}
        &P(a \leq x \leq b) = \int_{a}^{b} f(x) dx\\
        &f(x) \geq 0\\
        &\int_{- \infty}^{\infty} f(x) dx = 1
    \end{align*}

    \subsection*{Cumulative Distribution Function}
    \begin{align*}
        &F(x) = P(X \leq x) = \int_{-infty}^{x}f(t)dt \text{ for all x}\\
        &P(X > a) = 1 - F(a)\\
        &P(a < X < b) = F(b) - F(a)
    \end{align*}
    \subsection*{Mean and Variance of Continuous Random Variable}
    Mean of f(x) is 
    \begin{align*}
        &\mu = E[X] = \int_{-\infty}^{\infty} x f(x) dx\\
        &E[g(x)] = \int_{-\infty}^{\infty} g(x) f(x) dx
    \end{align*}
    Variance is 
    \begin{align*}
        \sigma^{2} &= Var(X) = E[(x - E[x])^{2}] = E[(x - \mu)^{2}]\\
        & = \int_{-\infty}^{\infty} (x-\mu)^{2} f(x) dx
    \end{align*}

    \subsection*{Properties of Mean and Variance}
    \begin{enumerate}
        \item $E(aX+b) = aE(x) + b$
        \item $E(X+Y) = E(X) + E(Y)$
        \item $E(XY) = E(X)E(Y) \text{ if X and Y are independent}$
        \item $Var(aX + b) = a^{2} Var(X)$
        \item $Var(X+Y) = VAR(X)+ VAR(Y) \text{ if X and Y are independent}$
        \item $Var(X_Y) = VAR(X) - VAR(Y) \text{ if X and Y are independent}$
    \end{enumerate}

    \section*{Some Continuous Random Variable Model}
    \subsection*{Uniform Distribution}
    If X is a uniform random variable, $X\sim U(a, b)$, X is evenly distributed on the interval [a,b].
    \begin{align*}
        &pdf = f(x) = \frac{1}{b-a}\\
        &\mu = E[X] = \frac{a+b}{2}\\
        &\sigma^{2} = \frac{(b-a)^{2}}{12}
    \end{align*}
    \subsection*{Exponential Distribution}
    Exponential random variables are often used to model the time until an event occur. A random variable $X exp(\lambda)$ has,
    \begin{align*}
        &pdf = f(x) = \lambda e^{-\lambda x}; x \geq 0 ; \lambda > 0\\
        &\mu = E[X] = \frac{1}{\lambda}\\
        &\sigma^{2} = \frac{1}{\lambda^{2}}
    \end{align*}
    where $\lambda$ is called the rate.
    
    \section*{Sum and Average of Independent Random Variable} 
        Let $X_{1}, X_{2}, X_{3}... X_{n}$ be n independent random variables. 
        $Y = a_{1}X_{1} + a_{2}X_{2}...a_{n}X_{n}$. \newline

    \begin{align*}
       &E[Y] = a_{1}E[X_{1}] + a_{2}E[X_{2}]...a_{n}E[X_{n}]\\
       &Var[Y] = a_{1}^{2}Var[X_{1}] + a_{2}^{2}Var[X_{2}]...+ a_{n}^{2}Var[X_{n}]
    \end{align*}

    If all a are 1, then 
    \begin{equation*}
        \bar{X} = (X_{1} + ... X_{n}) / n\\
    \end{equation*}

    \section*{Max and Min of Independent Random Variables}
    The maximum(V) and the minimum($\wedge$) of a sequence of n independent random variables can be used to 
    model a number of random quantities.

    \subsection*{Maximum}
    \begin{equation*}
        V = max\{X_{1}, X_{2},...X_{n}\}
    \end{equation*}
    V can be used to model
    \begin{enumerate}
        \item The lifetime of a system of n components connected in parallel
        \item The completion time of a project made up of n subprojects which can be pursued simultaneously
        \item The maximum flood level of a river in the next n years
    \end{enumerate}

    \begin{align*}
        F_{V}(v) = P(V \leq v) &= P(X_{1}\leq v, X_{2} \leq v \dots X_{n} \leq v )\\
        &= F_{1}(v)F_{2}(v)...F_{n}(v)
    \end{align*}
        where $P(x_{i}\leq v)  = F_{i}(v)$
    
    \subsection*{Minimum}
    \begin{equation*}
        \wedge = min\{X_{1}, X_{2},...X_{n}\}
    \end{equation*}
    $\wedge$ can be used to model
    \begin{enumerate}
        \item The lifetime of a system of n components connected in series
        \item The completion time of a project independently pursued by n competing teams
        \item The minimum flood level of a river in the next n years
    \end{enumerate}

    \begin{align*}
            F_{\wedge}(u) = P(\wedge \leq v) &= 1 - P(\wedge > u) = 1- P(X_{1}\geq u, X_{2} \geq u \dots X_{n} \geq u )\\
        &= 1- ((1-F_{1}(u))(1-F_{2}(u))...(1-F_{n}(u))
    \end{align*}
    
    \section{Normal Distribution}
        \section*{Standard Normal}
    When $a = 1/\sigma$ and $b = -\mu / \sigma$, 
    \begin{align*}
            &Z = \frac{x-\mu}{\sigma}\\
            &\mu = 0\\
            &\sigma^{2} = 1
    \end{align*}

    The standard normal density is denoted by $\phi$, the standard normal distribution is denoted by $\Phi$.
    \begin{align*}
            &\phi(z) = \frac{1}{2\pi} e^{\frac{-z^{2}}{2}}\\
            &\Phi(z) = \int_{-\infty}^{z} \frac{1}{2 \pi } e^{\frac{-t^{2}}{2}} dt\\
            &\Phi(-z) = 1 - \Phi (Z)
    \end{align*}

    Normal density cannot be integrated in close form, therefore tables are used. 
    \begin{equation*}
        F(x) = P(X \leq x) = P(z < \frac{x-\mu}{\sigma}) = \Phi(\frac{x-\mu}{\sigma})
    \end{equation*}
    \section*{The 68-95-99.7 Rule}
    In the normal distribution with mean $\mu$ and standard deviation $\sigma$,
    \begin{itemize}
        \item approximately 68\% of observation falls within $\sigma$ of $\mu$
        \item approximately 95\% of observation falls within $2\sigma$ of $\mu$
        \item approximately 99.7\% of observation falls within $3\sigma$ of $\mu$
    \end{itemize}
    
    \section*{Properties of normal distribution}
    \begin{enumerate}
        \item If $X\sim N(\mu, \sigma^{2})$ and $Y = aX+b$ then $Y\sim N(a\mu+b, a^{2}\sigma^{2})$.
        \item Suppose $X_{1} \dots X_{n}$ are independent normal random variables. $X_{i} \sim N(\mu_{i}, \sigma_{i}^{2})$, 
                    If Y is a linear combination of X, then $Y\sim N(a_{1}\mu_{1}+...a_{n}\mu_{n}, a_{1}^{2}\sigma_{1}^{2}+\dots a_{n}^{2}\sigma_{n}^{2})$.
            \item If $X_{1} \dots X_{n}$  are independent, identically distributed random variable with mean $\mu$ and std $\sigma$.Then 
                    $\bar{X} = N(\mu, \sigma^{2} / n)$. Where $\bar{X}$ is a linear combination of all X.
    \end{enumerate}

    \section*{Some Probability Model}
    \subsection*{Bernoulli's Ditribution}
        Bernoulli's random variable is a random variable that takes value of 1 in case of success and 0 in case of failure.
        \begin{align*}
            &X(success) = 1\\
            &X(failure) = 0
        \end{align*}
        \begin{align*}
           &X\sim Bernoulli(P) \\
           &P(X = x) = P^{x}(1-P)^{1-p}; x = 0, 1 \dots\\
        \end{align*}
        where p is the probability of success.
        \begin{align*}
            &E[X] = P\\
            &Var[X] = p(1-p)
        \end{align*}
        

        
    \subsection*{Binomial Distribution}
    A binomial random variable is the number of success for n independent trials.
    \begin{align*}
        &X\sim Bin(n, p) \text{ if the discrete random variable X has the pmf}\\
        &P(X=x) = {n \choose x} p^{x}(1-p)^{n-x};\, x = 1, 2, 3,...n
    \end{align*}
    where n is the number of trials and p is the probability of success.
    \begin{align*}
        &E[X] = np\\
        &Var[X] = np(1-p)
    \end{align*}
    \subsection*{Motivation for binomial}
    Consider performing an experiment n times where the probability of success in every trial is p 
    and the n trials are independent. We are interested in the probability of X successes. The probability 
    of getting X successes and n-x failures in the specific order is $P^{x}(1-p)^{n-x};\, x = 0, 1, 2, n$. 
    Since there are ${n \choose x} = \frac{n!}{x!(n-x)!}$ ways the trial outcome can be ordered, $P(x successes) = {n\choose x}p^{x}(1-p)^{n-x}$.

    \subsection*{Poisson's Distribution}
    Poisson's Distribution expresses the probability of a given number of events occurring in a 
    fixed interval of time or space if these events occurs with a known constant rate and independently.
    \begin{align*}
        &\text{X - the number of occurances in a given inteval or space}\\
        &X\sim Poisson(\lambda)\\
        &pmf = P(X=x) = \frac{\lambda^{x}e^{-\lambda}}{x!}\, x = 1, 2, 3, \dots\\
    \end{align*}
    where $\lambda$ is the rate of occurrences.
    \begin{align*}
       e^{k} = 1 + k + k^{2} / (2!) ... = \sum_{0}^{\infty} \frac{k^{x}}{x!}\\
       E(X) = Var(X)= \lambda
    \end{align*}

    Rules
    \begin{itemize}
        \item  The probability of an event within a certain interval does not change over other intervals.
        \item The umber of occurrences of an event in an interval is proportional to the size of the inteval.
        \item Events cannot happen simultaneously
    \end{itemize}
    \subsection*{Poisson's Distribution to estimate binomial}
    If $X\sim Bin(n, p)$ with large n($n \geq 20$) and small p($np < 5$), then we can use Poisson's random variable with rate 
    $\lambda = np$ to approximate binomial.
    \begin{equation*}
        Bin(n, p) = Poisson(np)
    \end{equation*}
   
    \subsection*{Exponential Random Variable}
    Let T be the time between two consecutive occurrences of an event. Then T is a continuous rv and it has exponential density.
    \begin{align*}
        &T\sim exp(\lambda)\\
        &f(t) = \lambda e^{-lambda t}\\
        &F(t) = 1-e^{-\lambda t}
    \end{align*}
    T is also the waiting time until the first event.

    \subsection*{Geometric Distribution}
    A geometric random variable counts the number of independent trials needed until the first success occurs where
    each trial has a probability of success.
    \begin{align*}
        &X\sim Geo(p)\\
        &f(x) = P(X = x) = P(1-p^{x-1}); \, x = 1, 2, \dots, n\\
        &F(x) = 1 - (1-p)^{x}; \, x = 1, 2, dots, n
    \end{align*}

    \begin{align*}
        &E[X] = \frac{1}{p}\\
        &Var(X) = \frac{1-p}{p^{2}}
    \end{align*}
    \section*{Normal Probability Approximations}
    \subsection*{Population and Sample}
    \begin{itemize}
        \item Population: The entire collection of individuals we want to study.
        \item Sample: A subset of individuals selected from the population.
    \end{itemize}

    Statistical techniques are used to make conclusions about the population 
    based on the sample.

    \subsection*{Statistic and Parameter}
    \begin{itemize}
        \item Statistic: A numerical summary of the sample. Ex. Sample mean, sample standard deviation.
        \item Sample: A numerical summary of the population. Ex. Population mean, population standard deviation.
    \end{itemize}

    Note that,
    \begin{itemize}
        \item Values of the parameter cannot be determined in practice.
        \item Due to sampling variability a statistic takes on different values for different samples.
        \item Parameters are estimated using sample date. Statistics is used to estimate parameters.
    \end{itemize}

    \subsection*{Sampling Distributions}
    The sampling distribution of a statistic is the probability distribution that specifies probabilities for the possible
    values the statistic can take.\par
    Sampling distributions describe the variability that occurs from study to study using statistics to estimate population parameters.\par
    Sampling distributions help to predict how close a statistic falls to the 
    parameter it estimates.

    If $X=[X_{1}, X_{2}, ... X_{n}]$ is a sample from a normal population with mean $\mu$ and 
    standard deviation $\sigma$, then

    \begin{equation*}
        \bar{X} = \frac{X_{1}+...X_{n}}{n} \sim N(\mu, \frac{\sigma^{2}}{n})
    \end{equation*}

    However, samples do not always follow a normal distribution. Suppose a random sample of 
    n observations is taken from a population with mean $\mu$ and standard deviation $\sigma$,
    then the mean of the mean of the samples is $\mu$ and the standard deviation of the mean of 
    the samples is $\frac{\sigma}{\sqrt{n}}$. The standard deviation of the samples mean is called the standard error.
    
    \section*{Central Limit Theorem}
    The CLT states that when an infinite number of successive random samples are taken from a population, the "sampling 
    distribution of the means of those samples will become approximately normally distributed with mean $\mu$ and 
    standard deviation $\frac{\sigma}{\sqrt{n}}$.

    Suppose we draw a random sample of size n, $X_{1}, X_{2}...X_{n}$ from a population random variable that is distributed
    with mean $\mu$ and standard deviation $\sigma$. Do this repeatedly and then calculate the mean of each sample. As the sample size
    increases, the distribution of the mean of the drawn samples will approach a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$.

    \begin{equation*}
        \bar{X} = \frac{X_{1}+...X_{n}}{n} \sim N(\mu, \frac{\sigma^{2}}{n})
    \end{equation*}

    The central limit theorem describes how the population mean and standard deviation 
    are related to the mean and the standard deviation of the mean of the samples.

    Then,
    \begin{equation*}
        \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0, 1)
    \end{equation*}

    \subsection*{Example}
    Closing prices of stocks have a right-skewed distribution with mean of $\$25$ and standard deviation of $\$20$.
    What is the probability that mean of a random sample of 40 stocks will be less than $\$20$?

    Consider the sampling distribution of sample mean. By CLT, 
    \begin{equation*}
        \bar{X} \sim N(25, \frac{20^{2}}{40})
    \end{equation*}

    \begin{align*}
        P(\bar{X} < 20) &= P(\frac{\bar{X} - \mu}{ \frac{\sigma}{\sqrt{n}}} < \frac{20-25}{\frac{20}{\sqrt{40}}})\\
        &= P(Z<-1.58)\\
        &= P(Z>1.58)\\
        &= 1-P(Z>1.58)\\
        &= 1-P(Z<1.58)\\
        &= \boxed{0.0571}
    \end{align*}

    \subsection*{Example}
    The time taken by a randomly selected applicant for a mortgage to fill out a certain form has a normal distribution
    with mean of 10 minute and standard deviation of 2 minute. If five individuals fill out a form on one day, what is the
    probability that the sample average amount of time taken on that day is at most 11 min?

    \begin{align*}
        &\mu = 10\\
        &\sigma = 2\\
        &n = 5\\
        &P(\bar{X} \leq 11) = ?
    \end{align*}

    \begin{align*}
        P(\bar{X} \leq 11) &= P(\frac{\bar{X} - \mu}{ \frac{\sigma}{\sqrt{n}}} \leq \frac{11-10}{\frac{2}{\sqrt{5}}})\\
        &=(Z\leq 1.12)\\
        &= 0.8686
    \end{align*}

    TODO!!!!!!!!!!!!!

    \section*{Normal Approximation to the Binomial Distribution}
    Let $X \sim Bin(n, p)$. When n is large so that both $np\geq 5$, we can use the normal distribution to 
    get an approximate answer.

    \begin{equation*}
        X \sim N(np, np(1-p))
    \end{equation*}

    * When we use normal approximation to the Binomial distribution, the \textbf{continuity correction} 
    should be used because we are approximating a discrete random variable with a continuous random variable.

    \subsection*{Example}
    Let $X \sim Bin(10, 0.5)$, obtain $P(x\leq 2)$,
    \begin{enumerate}
        \item exactly
        \item using the normal approximation
        \item using the normal approximation with a continuity correction
    \end{enumerate}

    \paragraph{1.}

    \begin{align*}
        &X \sim Bin(10, 0.5)\\
        &n = 10\\
        &p = 0.5
    \end{align*}

    \begin{align*}
        P(X \leq 2) &=  P(x = 0) + P(x = 1) + P(x = 2)\\
        & = {10 \choose 0 }(0.5)^{0}(0.5)^{10} + {10 \choose 1} (0.5)^{1}(0.5)^{9} + {10 \choose 2} (0.5)^{2}(0.5)^{8}\\
        & = 0.0547
    \end{align*}

    \paragraph{2.}
    \begin{align*}
        &n = 10\\
        &p = 0.5\\
        &np = 5 \geq 5\\
        &n(1-p) \geq 5
    \end{align*}

    Therefore $X \sim N(5, 2.5)$,

    \begin{align*}
        P(x\leq 2) &= P(\frac{x - \mu}{\sigma} \leq \frac{2-5}{\sqrt{2.5}})\\
        &= P(Z\leq-1.9) = 0.0287
    \end{align*}

    This is not so good because the exact answer is 0.0547.

    \paragraph{3.}

    \begin{align*}
        P(x \leq 2) &= P(x \leq 2 + 0.5) = P(\frac{x - \mu}{\sigma} \leq \frac{2.5-5}{\sqrt{2.5}})\\
        &= P(Z\leq-1.58)\\
        &= 0.0571
    \end{align*}
    
    0.5 is the continuity correction. The end result is closer to the exact answer.

    \section*{Continuity Correction}
    If x is a discrete random variable and y is a continuous random variable.
    \begin{align*}
        &P(x>4) = P(x\geq 5) = P(y \geq 4.5)\\
        &P(x \leq 4) = P(y \leq 4.5)\\
        &P(x<4) = P(x \leq 3) = P(y \leq3.5)\\
        &P(x \leq 4) = P(y \leq 4.5)\\
        &P(x = 4) = P(4-0.5 \leq y \leq 4+0.5)
    \end{align*}

    \section*{Normal Approximation To The Poisson Distribution}
    If $X \sim Poisson(\alpha)$ where $\alpha$ is the expected number of counts.
    When $\alpha$ is large ($\geq 20$), then normal distribution can be used to approximate the
    Poisson distribution.
    
    \begin{equation*}
        X \sim N(\alpha, \alpha)
    \end{equation*}
    
    \subsection*{Example}
    A radioactive element disintegrates such that it follows a Poisson distribution. The
    mean number of particles emitted is recoded in 1 second interval is 55. Find the probability of

    \begin{enumerate}[label={(\alph*)}]
        \item more than 60 particles are emitted in 1 second
        \item  between 50 and 65 particles inclusive are emitted in 1 second
    \end{enumerate}

    Let $X = \#$ of particles emitted in 1 second.
    \begin{equation*}
        X \sim Poisson(55)
    \end{equation*}

    Since $\lambda $ is large($\geq 20$), $X \sim N(55, 55)$.

    \paragraph{(a)}
    \begin{align*}
    P(X > 60) &= P(X \geq 61)\\
    &=P(X \geq 60.5)\\
    & = P(\frac{x-\mu}{\sigma} \geq \frac{60.5 - 55}{\sqrt{55}})\\
    & = P(Z \geq 0.74)\\
    & = 1 - P(Z < 0.74)\\
    & =1-0.7704\\
    & = 0.2296
    \end{align*}

    \paragraph*{(b)}
    \begin{align*}
        P(50 \leq x \leq 65) &= P(49.5 \leq x \leq 65.5)\\
        & = P(\frac{49.5-55}{\sqrt{55}} \leq \frac{x-\mu}{\sigma} \leq \frac{65.5-55}{\sqrt{55}})\\
        & = P(-0.74 \leq Z \leq 1.41)\\
        & = 0.6911
    \end{align*}

    \section*{Sum of Random Samples With CLT}
    Consider a random sample $X_{1}, X_{2}... X_{n}$ from a distribution with mean $\mu$ and variance
    $\sigma^{2}$. When n is large, by CLT

    \begin{equation*}    
          \bar{X} = \frac{X_{1}+...X_{n}}{n} \sim N(\mu, \frac{\sigma^{2}}{n})
    \end{equation*}

    If a question is about sum instead of an average, CLT can still be used.
    Let $T = X_{1} + X_{2} + .... X_{n}$,
    \begin{align*}
        E[T] = n\mu\\
        Var[T] = n\sigma^{2}\\
        T \sim N(n\mu, n\sigma^{2})
    \end{align*}

    \section*{Statistical Modeling and Inference}
    \subsection*{Statistical Inference}
    Method of making decisions or predictions about a population 
    based on information obtained from a sample.\par

    The objective of estimation is to determine the approximate value
    of a population parameter on the bases of a sample statistic.\par

    Sample mean ($\bar{X}$) is used to estimate the population mean($\mu$).\par

    \subsection*{Two Types of Estimators}
        \begin{itemize}  
            \item Point Estimator - draws inferences about a population 
                                    by estimating the value of an unknown parameter using a single value
                                    or point.
            \item Interval Estimator - draws inferences about a population
                                    by estimating the value of an unknown parameter using an interval.
                                    The population parameter of interest is between some lower and upper bounds.
        \end{itemize}

        \subsubsection*{Point Estimate vs Interval Estimate}
        A point estimate does not tell us how close the estimate is likely to be to the parameter.
        An interval estimate is usually more useful. 

    \section*{Point Estimators}
    A good estimator has a sampling distribution that
    is centered at the parameter(unbiasedness).\par

    An unbiased estimator of a population parameter is an estimator
    whose expected value is equal to that parameter.\par

    \begin{equation*}
        E[\hat{\theta}] = \theta
    \end{equation*}
    where $\theta$ is the parameter and $\hat{\theta}$ is a point estimator.\par

    A good estimator has a small standard error compared to the other estimators.\par

    \subsection*{Example}
    Suppose we want to estimate the mean summer income of a class of statistics students.
    For a sample of 30 students sample mean($\bar{X}$) is calculated to be $\$500$ per week.\par

    Point estimate for population mean($\mu$) income per week is $\hat{\mu} = \bar{X} = \$500$(point estimate).
    An alternative statement is: "The mean income is between $\$400 \sim \$600$ per week(interval estimate).

    \subsection*{Example}
    Suppose that $X_{1} , X_{2}... X_{n}$ is a random sample from a population with mean $\mu$
    and variance $\sigma^{2}$.\par
   
    \begin{itemize}
        \item $\bar{X}$ is an unbiased estimator of $\mu$
            \begin{equation*}
                E[\bar{X}] = E[\frac{\sum{X_{i}}}{n}] = \mu
            \end{equation*}
        \item $S^{2}$ is an unbiased estimator of $\sigma^{2}$
        \begin{equation*}
            E[S^{2}]  = \sigma^{2}
        \end{equation*}
    \end{itemize}

    \section*{Confidence Interval for $\mu$}
    Consider the absolute estimation error $|\bar{Y} - \mu|$ (sample mean - population mean). 
    We wish to find a value d such that there is a large probability (0.95 or 0.99) that the absolute estimation 
    error is below d.

    \begin{equation*}
        \text{Confidence Level} = P(|\bar{Y} - \mu| < d) = 1 - \alpha
    \end{equation*}
    where $\alpha$ is typically 0.05(95$\%$ confidence) or 0.01($99\%$ confidence). Assume 0.005 if not defined. \par

    The resulting d can be added or subtracted from the observed average $\bar{y}$ to obtain 
    the upper and lower limits of an interval called ($1-\alpha$) $100\%$ confidence interval.
    \begin{equation*}
        (\bar{y} - d, \bar{y} + d)
    \end{equation*}

    \subsection*{Example}
    \begin{align*}
        &\bar{X} \sim N(\mu, \frac{\sigma^{2}}{n})\\
        &Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}\\
        &P(-1.96 \leq \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \leq 1.96) = 0.95\\
        &P(\bar{X} - 1.96\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + 1.96\frac{\sigma}{\sqrt{n}}) = 0.95
    \end{align*}

    Therefore, 95$\%$ confidence interval for $\mu$ is 
    \begin{equation*}
        [\bar{X} - Z_{0.025}\frac{\sigma}{\sqrt{n}}, \bar{X} + Z_{0.025}\frac{\sigma}{\sqrt{n}}]
    \end{equation*}
    
    \subsection*{Example}
    The average zinc concentration recovered from a sample of measurements taken in 
    36 different locations in a river is found to be 2.6g per ml. Find the $95\%$ and $99\%$ confidence interval for 
    the mean zinc concentration in the river. Assume that the population standard deviation is 0.3g/ml.

    \begin{align*}
        \bar{X} = 2.6\\
        n = 36\\
        \sigma = 0.3
    \end{align*}

    \paragraph*{95$\%$ CI for $\mu$}
        \begin{align*}
            \bar{X} \pm Z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\\
            [2.507, 2.693]
        \end{align*}

    \paragraph*{$99\%$ CI for $\mu$}
        \begin{equation*}
             [2.471, 2.729]
        \end{equation*}

    \section*{CI when $\sigma$ is unknown}
        $\sigma$ is estimated by the sample standard deviation S.
        This estimation introduces extra error. To account for this error,
        z-score is replaced by a slightly larger score called the t-score.

    \section*{Interpreting a CI for $\mu$}
    If we repeatedly obtain samples of size n and construct the corresponding $95\%$ 
    confidence interval for $\mu$, on average, $95\%$ of these intervals will include the value of $\mu$.

    \subsection*{Example}
    A reporter is writing an article on the cost of 
    off-campus housing. A sample of 16 studio apartments within 3 km of campus 
    resulted in a sample mean $\$1000/month$ and sample standard deviation of $\$150$.
    Assuming population to be normal, calculate $95\%$ CI for the population mean rent per month.

    \begin{align*}
        n = 16\\
        \bar{X} = 1000\\
        S = 150
    \end{align*}

    \begin{align*}
        \bar{X} \pm t_{0.025, n-1}\frac{s}{\sqrt{n}}\\
        1000 \pm 2.131 \frac{150}{\sqrt{16}}\\
        [920, 1080]
    \end{align*}

    \section*{Testing Hypothesis About $\mu$}
    Hypothesis testing can be used to determine whether 
    a statement about the value of a population parameter should or 
    should not be rejected.

    \section*{Null and Alternative Hypothesis}
    The null hypothesis, denoted by $H_{o}$, is a tentative assumption 
    about a population parameter.\par

    The alternative hypothesis, denoted by $H_{a}$ is the opposite of what is 
    stated in the null hypothesis. 
    The alternative hypothesis is what the test is attempting to establish. \par
    
    The equality part of the hypothesis always appears in the null hypothesis.\par

    Hypothesis test about the value of a population mean 
    $\mu$ must take one of the 3 forms.

    \begin{itemize}
        \item $H_{o}: \mu \geq \mu_{o} \text{ vs } H_{a}: \mu < \mu_{o}$
        \item $H_{o}: \mu \leq \mu_{o} \text{ vs } H_{a}: \mu > \mu_{o}$
        \item $H_{o}: \mu = \mu_{o} \text{ vs } H_{a}: \mu \neq \mu_{o}$
    \end{itemize}
    where $\mu_{o}$ is the hypothesized value of the population mean.\par

    The hypothesis should be formulated before viewing or analyzing the data.

    \section*{Test Procedures}
    A test procedures is specified by the following.

    \begin{itemize}
        \item Test Statistic: a function of the sample data on which the decision
        (Rejected $H_{o}$ or do not reject $H_{o}$) is to be based.
        \item Rejection Region: the set of all test statistic values for which $H_{o}$ will be rejected.
    \end{itemize}

    A test statistic is constructed assuming the null hypothesis is correct.

    \paragraph*{Case 1: $\sigma$ is known}
    test statistic is,
    \begin{equation*}
        Z = \frac{\bar{X} - \mu_{o}}{\frac{\sigma}{\sqrt{n}}} \sim N(0, 1)
    \end{equation*}

    \paragraph*{Case 2: $\sigma$ is unknown}
    test statistic is,
    \begin{equation*}
        t = \frac{\bar{X} - \mu_{o}}{\frac{s}{\sqrt{n}}} \sim t_{n-1}
    \end{equation*}
    When n is large,
    \begin{equation*}
        t = \frac{\bar{X} - \mu_{o}}{\frac{s}{\sqrt{n}}} \sim t_{n-1} \sim N(0,1)
    \end{equation*}

    The null hypothesis will be rejected if and 
    only if the observed or computed test statistic value falls in the rejection region.\par

    We also use the test statistic to assess the evidence against the 
    null hypothesis by giving a probability, p-value.

    \section*{P-Value}
    The P-Value summarizes the evidence. It describes how unusual the 
    data would be if $H_{o}$ were true.\par
    
    P-Value is defined as the probability of observing a result as extreme 
    or more extreme than what we observed given that $H_{o}$ is true.

    \section*{Significance Level ($\alpha$)}
    The Significance level is a predetermined number such that we reject $H_{o}$ if the P-Value 
    is less than or equal to that number.\par

    In practice, the most common significance level is $\alpha = 0.05$.\par

    When we reject $H_{o}$, we say the results are statistically significant.

    \begin{itemize}
        \item if $p-value \leq \alpha$ $=>$ Reject $H_{o}$
        \item if $p-value > \alpha$ $=>$ Do not reject $H_{o}$
    \end{itemize}

    \section*{Steps of Hypothesis Testing}
        \begin{enumerate}
            \item Develop the null and alternative hypothesis
            \item Specify the level of significance $\alpha$
            \item Collect the sample data and compute the test statistic
        \end{enumerate}

        Then, 2 approaches can be used.
        \paragraph*{p-value approach}
        \begin{enumerate}
            \item Use the value of the test statistic to compute the p-value
            \item Reject $H_{o}$ if $p-value \leq \alpha$
            \item Conclusion
        \end{enumerate}

        \paragraph*{Critical Value Approach}
        \begin{enumerate}
            \item Use the level of significance to determine the 
                    critical value and the rejection rule
            \item Use the value of the test statistic and rejection rule to determine whether to reject $H_{o}$
            \item Conclusion
        \end{enumerate}

        \section*{Decisions and Types of Errors in Hypothesis Testing}
        \begin{itemize}
            \item $H_{o}$ is true / Reject $H_{o}$ $=>$ type I error
            \item $H_{o}$ is false / Do not reject $H_{o}$ $=>$ type II error
        \end{itemize}

        \paragraph*{What is a good test?\newline}
        A test that rarely makes type I and type II errors.

        \begin{align*}
            \text{P(Type I error)} = \alpha \\
            \text{P(Type II error) }= \beta
        \end{align*}

        We can control the probability of type I error by our choice of the significance level $\alpha$.\par

        It is difficult to control the probability of making type II error.\par

        Statisticians avoid the risk of making a type II error by using "do not reject $H_{o}$" 
        and not "accept $H_{o}$"\par

        $1-\beta$  is referred to as the power of a test.The greater the power, 
        the less likely type II error occurs.\par

        $\alpha$ and $\beta$ are test properties, and they are independent of data.

        \section*{Power of a Test}
        Power is the probability of correctly rejecting the null hypothesis 
        $H_{o}$, when $H_{o}$ is false. 

        \section*{Tail Test}
        Use two tail test when $\mu = \mu_{o}$; use one tail test when $\mu \geq \mu_{o}$.
        \begin{figure}[H]
            \centering
            \includegraphics[width=10cm]{figures/tail_test.PNG}
            \caption{Tail Tests}
            \label{fig:tail_test}
        \end{figure}


        \subsection*{Example}
        A department store manager determines that a new billing system will be cost-effective 
        only if the mean monthly account is more then $\$170$.\par

        A random sample of 400 monthly accounts is drawn, from which the sample mean 
        is $\$178$. The accounts are approximately normally distributed with $\sigma = \$65$.

        \begin{enumerate}[label={(\alph*)}]
            \item Can we conclude that the new system will be cost effective? (use $\alpha = 0.05$)?
            \item Describe what type I and type II errors in the content of this problem situation
            \item Considering the test procedure, find the rejection region of $\bar{X}$
            \item When $\mu = 180$, find the probability of type II error.
            \item Evaluate the power of the test when $\mu = 180$
        \end{enumerate}

        \subsubsection*{(a)}
        \begin{align*}
            &n = 400\\
            &\bar{X} = 170\\
            &\sigma = 65
        \end{align*}

        \begin{align*}
            &H_{o} = \mu \leq 170  \rightarrow \text{ use right tail test}\\
            &\mu_{o} = 170\\
            &H_{a} = \mu > 170\\
        \end{align*}

        \begin{align*}
            Z_{obs} &= \frac{\bar{X}-\mu_{o}}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)\\
            &= \frac{178-170}{\frac{65}{\sqrt{400}}} = 2.46
        \end{align*}
        \paragraph*{Method1:Critical Value Approach}
        Since $Z_{obs}= 2.46 > Z_{0.05} = 1.645$, we reject $H_{o}$. The conclusion is that 
        the new system is cost-effective.\par
        
        \paragraph*{Method2:P-value Approach}
        P-Value = P(observing data as extreme or more extreme than what we observed, given $H_{o}$ is true)
        \begin{align*}
            P(\bar{X} \geq 178 \text{ when } \mu =  170)\\
            = P(Z \geq 2.46) = 0.0069
        \end{align*}
        Since $0.0069 < \alpha = 0.05$, $H_{o}$ is rejected, the conclusion is that the new system is cost 
        effective.

        \subsubsection*{(b)}

        Type I error: reject $H_{o}$ when $H_{o}$ is true.

        Conclude that the new billing system is cost-effective when it is not(i.e. true mean $>$ 170).\par

        Type II error: do not reject $H_{o}$ when $H_{o}$ is false.

        Conclude that the new billing system is not cost-effective when it is.

        \subsubsection*{(c)}
        Reject when $Z > 1.645$.
        \begin{align*}
            Z = \frac{\bar{X}- \mu_{o}}{\frac{\sigma}{\sqrt{n}}}\\
            \bar{X} > 175.35
        \end{align*}
        Therefore, rejection region is $\bar{X} > 175.35$.

        \subsubsection*{(d)}
        \begin{equation*}
            \mu = 180
        \end{equation*}
        When $\mu=180$, $\bar{X}$ has a normal distribution with mean 180 and sigma of $\frac{65}{\sqrt{400}}$.
        \begin{align*}
            \beta = \text{P(type II error)} &= \text{P(do not reject $H_{o}$ when $H_{o}$ is false)}\\
            &= P(\bar{X} < 175.35 \text{ when $\mu = 180$})\\
            &= P(\frac{\bar{X} - 180}{\frac{65}{\sqrt{400}}} < \frac{175.35 - 180}{\frac{65}{\sqrt{400}}})\\
            &= P(Z < -1.43)\\
            &= 0.0764
        \end{align*}

        \subsubsection*{(e)}
        \begin{align*}
            \text{Power} &= \text{P(Reject $H_{o}$ when $H_{o}$ is false)}\\
            &= 1-\beta\\
            &= 1-0.0764\\
            &= 0.9236
        \end{align*}
        This is a very powerful test since it makes the correct decision 92.36$\%$ of the time when $\mu = 180$.

        \section*{Two Sample Problems}
        Compare the means of two independent populations, assuming equal population standard deviations.\par

        *Suppose we draws a random sample from each of the two independent populations 
        with means $\mu_{1}$, $\mu_{2}$, and standard deviations of $\sigma_{1}$ and $\sigma_{2}$.\par

        Hypotheses take one of the following 3 forms.
        \begin{itemize}
            \item Left-Tailed
                \begin{align*}
                    H_{o}: \mu_{1} -  \mu_{2} \geq \Delta_o\\
                    H_{a}: \mu_{1} -  \mu_{2} < \Delta_o
                \end{align*}
            \item Right-Tailed
                \begin{align*}
                    H_{o}: \mu_{1} -  \mu_{2} \leq \Delta_o\\
                    H_{a}: \mu_{1} -  \mu_{2} > \Delta_o
                \end{align*}
            \item Two-Tailed
                \begin{align*}
                    H_{o}: \mu_{1} -  \mu_{2} = \Delta_o\\
                    H_{a}: \mu_{1} -  \mu_{2} \neq \Delta_o
                \end{align*}
        \end{itemize}

        \subsection*{Example}

        If the Hypotheses are
            \begin{align*}
                H_{o}: \mu_{1} \geq  \mu_{2} \rightarrow \mu_{1} - \mu_{2} \geq 0\\
                H_{a}: mu_{1} <  \mu_{2}  \rightarrow \mu_{1} - \mu_{2} < 0
            \end{align*}
        In this case, $\Delta = 0$.

        \subsection*{Assumptions}
        \begin{itemize}
            \item random samples from each of the population is drawn
            \item the sample individuals are independent of each other
            \item both populations are normal or we need reasonably large samples to validate using CLT
            \item both population distributions have equal variance ($\sigma_{1}^{2} = \sigma_{2}^{2}$)
        \end{itemize}
       
        \subsection*{Test Statistic}
        We select a simple random sample of size $n_{1}$, from population 1 and a simple random sample 
        of size $n_{2}$.\par
        Let $\bar{X_{1}}$ be the mean of sample 1 and $\bar{X_{2}}$ be the mean of sample 2.\par

        Test statistic t is.
        \begin{equation*}
            t = \frac{(\bar{X_{1}} - \bar{X_{2}})-\Delta_{o}}{S_{p} \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}}
        \end{equation*}
        where $S_{p}$ is the pooled standard deviation.

        \subsection*{The Pooled Standard Deviation}
        This method requires the assumption that population variances are equal.
        \begin{equation*}
            \sigma_{1}^{2} =  \sigma_{2}^{2} =  \sigma^{2}
        \end{equation*}

        $S_{p}$, the pooled standard deviation estimates the common value $\sigma$.
        \begin{equation*}
            S_{p} = \sqrt{\frac{(n_{1}-1)S_{1}^{2}+(n_{2} - 1) S_{2}^{2}}{n_{1}+n_{2} -2}}
        \end{equation*}

        $(1-\alpha)100\%$ confidence interval for the difference between two population means (i.e. $\mu_{1} - \mu_{2}$).\par

        \begin{itemize}
            \item Point Estimator of $\mu_{1} - \mu_{2}$ is $\bar{X_{1}} - \bar{X_{2}}$
            \item CI $\rightarrow$ point estimate $\pm$ margin of error.
            \item $(1-\alpha)100\%$ CI for $\mu_{1} - \mu_{2}$ is 
                  \begin{equation*}
                    (\bar{X_{1}}- \bar{X_{2}}) \pm t_{\frac{\alpha}{2}, n_{1} + n_{2} - 2} \cdot S_{p} \cdot \sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}
                  \end{equation*}
        \end{itemize}

        \subsection*{Example}
        Average densities of two types of brick are compared. (Type A and Type B).
        \subsubsection*{a)}
        Using the following sample data, test the claim that the 
        true mean densities are equal.
        \subsubsection*{b)}
        Use a 0.05 significance level and assume normality of the two density distribution and equal population variances.
        \subsubsection*{c)}
        Calculate $95\%$ CI for $\mu_{A} - \mu_{B}$.

        \begin{center}
            \begin{tabular}{c c}
            Type A & Type B   \\
            $n_{A} = 8$ & $n_{B} = 10$ \\
            $\bar{X_{A}} = 22.7$ & $\bar{X_{B}} = 21.5$ \\
            $S_{A} = 0.8$ & $S_{B} = 0.6$
            \end{tabular}
        \end{center}

        Let $\mu_{A}$ be the true average density for type A and $\mu_{B}$
        be the true density for Type B.

        \subsubsection*{Hypotheses}
            \begin{align*}
                H_{o}: \mu_{A} = \mu_{B} \rightarrow \mu_{A} - \mu_{B} = 0\\
                H_{a}: \mu_{A} \neq \mu_{B}  \rightarrow \mu{A} = \mu_{B} \neq 0
            \end{align*}
            Use two tail test and $\alpha = 0.05$.

        \subsubsection*{Pooled Standard Deviation}
            \begin{align*}
                S_{p}^{2} &= \frac{ (n_{A} - 1)S_{A}^{2} + (n_{B} - 1) S_{B}^{2} }{n_{A} + n_{B} - 2}\\
                &=  \frac{ (8 - 1)(0.8)^{2} + (10 - 1) (0.6)^{2} }{8 + 10 - 2}\\
                &= 0.4825\\
                &S_{p} = 0.695
            \end{align*}
        \subsubsection*{Test Statistic}
             \begin{align*}
                t_{obs} &= \frac{(\bar{X_{A}} - \bar{X_{B}})-0}{S_{p} \sqrt{\frac{1}{n_{A}}+\frac{1}{n_{B}}}} \sim t_{n_{1}+n_{2}-2}\\
                &= \frac{22.7-21.5}{0.695 \sqrt{\frac{1}{8} +\frac{1}{10}}}\\
                &= 3.64
             \end{align*}
       \subsubsection*{Critical Value Approach} 
       \begin{align*}
        &\alpha = 0.05\\
        &\alpha/2 = 0.025\\
        &t_{0.025, 16} = 2.12\\
       \end{align*}

       Calculated test statistic value is in the rejection region.
       \begin{equation*}
        |t_{obs}| = 3.64 > t_{0.025, 16} = 2.12 \rightarrow \text{Reject } H_{o} \text{ at } \alpha = 0.05
       \end{equation*}

       \subsubsection*{Conclusion}
       At the significant level 0.05, we conclude that 
       the true mean densities of two types of brick are not equal.

       \subsubsection*{c)}
            \begin{align*}
                &(\bar{X_{A}}- \bar{X_{B}}) \pm t_{\frac{\alpha}{2}, n_{A} + n_{B} - 2} \cdot S_{p} \cdot \sqrt{\frac{1}{n_{A}}+\frac{1}{n_{B}}}\\
                &(22.7 - 21.5) \pm 2.12 (0.695) \sqrt{\frac{1}{8}+\frac{1}{10}}\\
                &1.2 \pm 0.33\\
                &[0.87, 1.53]
            \end{align*}

        \subsubsection*{Hypotheses}
            \begin{align*}
                H_{o}: \mu_{A} - \mu_{B} = 0\\
                H_{a}: \mu_{A} - \mu_{B} \neq 0
            \end{align*}

            The calculated confidence interval does not contain the hypothesized value.
            Therefore we can reject the null hypothesis, $H_{o}$.

            \section*{Comparison of Several Means}
            \subsection*{Analysis of Variance ANOVA}
            ANOVA is a statistical method that tests the equality of three or more population 
            means by analyzing sample variances or variation in the data.\par
            he simplest ANOVA problem is referred to variously as a single-factor, single-classification, 
            or one-way ANOVA.

            \subsection*{Example}
            \begin{enumerate}
                \item An experiment to study the effect of five different brands of gasoline 
                        on automobile engine operating efficiency (mpg)
                \item An experiment to study the effect of the presence of three 
                        different sugar solutions on bacterial growth.
            \end{enumerate}

            \subsection*{One-Way ANOVA}
            One-way ANOVA focuses on a comparison of 3 or more population or 
            treatment means.\par

            Let k be the number of populations or treatments being compared.
            \begin{align*}
                &\mu_{1} \text{ is the mean of population 1 or the true average response when 
                treatment 1 is applied.}\\
                &\text{.}\\
                &\text{.}\\
                &\text{.}\\
                &\mu_{k} \text{ is the mean of population k or the true average 
                response when treatment k is applied.}
            \end{align*}

            \subsection*{Hypotheses}
            \begin{align*}
                H_{o}: \mu_{1} = \mu_{2} = ...= \mu_{k}\\
                H_{a}: \text{at least two of the $\nu_{i}$ are different}
            \end{align*}
            Reject $H_{o}$ here means that at least two population means have 
            different values.

            \subsection*{Assumptions for ANOVA}
            For each population, the response variable is normally distributed.\par
            The variance of the response variable, denoted $\sigma^{2}$, is the same for all the 
            populations.\par
            The observations must be independent.
            
            \subsection*{Notation}
            k random samples observed.
            $y_{ij}$ is the $j_{th}$ observed value from the $i^{th}$ population.

            \begin{figure}[H]
                \centering
                \includegraphics[width=10cm]{figures/anova_table.PNG}
                \label{fig:anova_table}
            \end{figure}

            where,
            \begin{equation*}
                \bar{y_{i}} = \frac{\sum_{j=1}^{ni} y_{ij}}{n_{i}} = \frac{y_{i0}}{n_{i}}
            \end{equation*}

            \begin{align*}
                \text{Total Sample Size } = n = n_{1} + ...n_{k}\\
                \text{Grand Total } = y_{oo} = \sum_{k}^{i = 1} \sum_{n_{i}}^{j=1}y_{ij}\\
                \text{Grand Mean } = \bar{y_{oo}} = \frac{y_{oo}}{n} = \frac{\sum_{k}^{i = 1} \sum_{n_{i}}^{j=1}y_{ij}}{n} 
            \end{align*}

            Let $Y_{ij}$ be the random variable that denotes the $j^{th}$ measurement 
            taken from the $i^{th}$ treatment.

            Then $y_{ij}$ is the observed value of $Y_{ij}$.

            \begin{align*}
                E[\bar{y_{i}}] = \mu_{i}\\
                Var(\bar{y_{i}}) = \frac{\sigma}{n_{i}}
            \end{align*}

            For k random samples, we can find calculate the sample variances.
            \begin{equation*}
                S_{1}^{2}, S_{2}^{2}...S_{k}^{2}
            \end{equation*}

            $S_{1}^{2}, S_{2}^{2}...S_{k}^{2}$ are k different 
            unbiased estimates for the common variance $\sigma^{2}$.
            \begin{equation*}
                E[S_{i}^{2}] = \sigma^{2}
            \end{equation*}
          
            These k estimates can be combined to obtain an 
            unbiased estimate for $\sigma^{2}$.
            \begin{equation*}
                s^{2} = \frac{\sum_{i=1}^{k} (n_{i}-1)S_{i}^{2}}{n-k} = MSE
            \end{equation*}
            where
            \begin{equation*}
                S_{i}^{2} = \frac{\sum_{i=1}^{n_{i}} (y_{ij} - \bar{y_{ij}})^{2}}{n_{i} - 1}
            \end{equation*}

            \subsection*{$H_{o}$ is true}
            Sample means are close together because there is only one sampling distribution.

            \subsection*{$H_{o}$ is false}
            Sample means comes from different sampling distributions and are not close together.

            \subsection{Total Variation In The Data (SSE - total sum of squares)}
            Comes from 2 sources.
            \begin{itemize}
                \item Variation between groups/treatments. (SSTr- Treatment sum of squares)
                \item Variation within groups/treatments.(SSE - Error sum of squares)
            \end{itemize}

            \begin{equation*}
                SST = SSTr + SSE
            \end{equation*}

            \begin{align*}
                &SST = \sum_{i=1}^{k}\sum_{j=1}^{n_{i}} (y_{ij}- \bar{y_{oo}})^{2} =  \sum_{i=1}^{k}\sum_{j=1}^{n_{i}}  y_{ij}^{2} - \frac{1}{n}y_{oo}^{2}\\
                &SSTr = \sum_{i=1}^{k}\sum_{j=1}^{n_{i}} (\bar{y_{io} - \bar{y_{oo}}})^{2} =  \sum_{i=1}^{k} \frac{1}{n_{i}} y_{i}^{2} - \frac{1}{n} y_{oo}^{2}\\
                &SSE =  \sum_{i=1}^{k}\sum_{j=1}^{n_{i}} (y_{ij} - \bar{y_{io}})^{2} =  \sum_{i=1}^{k}\sum_{j=1}^{n_{i}} y_{ij}^{2} - \sum_{i=1}^{k} \frac{y_{io}^{2}}{n_{i}} = \sum_{i=1}^{k} (n_{i} - 1)S_{i}^{2}
            \end{align*}

            \subsection*{Degrees of Freedom}
            \begin{align*}
                df(SST) = n-1\\
                df(SSE) = n-k\\
                df(SSTr) = k-1
            \end{align*}

            \subsection*{Mean Squares}
            \begin{align*}
                \text{Mean Square Treatment} = MSTr = \frac{SSTr}{k-1}\\
                \text{Mean Square Error} = MSE = \frac{SSE}{n-k}
            \end{align*}
            *MSE is a measure of with-sample variation.

            \subsection*{ANOVA Test Procedure}
            \begin{align*}
                &H_{o}: \mu_{1} = \mu_{2} = \dots = \mu_{k}\\
                &H_{a}: \mu_{i} \neq \mu_{j} \text{ for } i\neq j
            \end{align*}

            \subsection*{Test Statistic}
            \begin{equation*}
                F_{obs} = \frac{MSTr}{MSE} \sim F_{\gamma_{1}, \gamma_{2}}
            \end{equation*}

            Under $H_{o}$, $F_{obs}$ follows the F-distribution with degrees of freedom,
            \begin{align*}
                \gamma_{1}(\text{numerator df}) = df(SSTr) = k-1\\
                \gamma_{2}(\text{Denominator df} = df(SSE) = n-k
            \end{align*}
            \subsection*{F-distribution}
            Reject $H_{o}$ if $F_{obs} \geq F_{\alpha}$.

            \subsection*{The ANOVA TABLE}
            \begin{figure}[H]
                \centering
                \includegraphics[width=10cm]{figures/anova_table2.PNG}
                \label{fig:anova_table2}
            \end{figure}
          
            \subsection*{The ANOVA Model}
            The assumptions of single-factor ANOVA can be described sufficiently 
            by means of the "model equation". \par
            Each measurement will be represented as the sum of 
            two terms, as unknown constant, $\mu_{i}$, and a random variable, $\epsilon_{ij}$.

            \begin{align*}
                &Y_{ij} = \mu_{i} + \epsilon_{ij}\\
                &i = 1, 2, \dots , k\\
                &j = 1, 2, \dots ,n_{i}
            \end{align*}
            where $\epsilon_{ij}$ represents a random deviation from the population or true treatment 
            mean $\mu_{i}$.

            \subsection*{The model assumptions are:}
            \begin{enumerate}
                \item Independence: The random variables $\epsilon_{ij}$ are independent (implying that $X_{ij}$ are also).
                \item Constant Treatment Means: $E(\epsilon_{ij}) = 0$ for all i and j
                \item Constant Variance: $Var(\epsilon_{ij}) = \sigma^{2}$ for all i and j
                \item Normality: The variables $\epsilon_{ij}$ are normal.
            \end{enumerate}
            


\end{document}

 